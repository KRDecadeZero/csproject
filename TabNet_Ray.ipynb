{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import os\n",
    "import tempfile\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial\n",
    "from ray.train import Checkpoint\n",
    "import warnings\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device is {device}!\")\n",
    "\n",
    "local_dir = os.path.abspath(\"./ray_tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dns_qtype</th>\n",
       "      <th>dns_rcode</th>\n",
       "      <th>dns_query</th>\n",
       "      <th>dst_ip_bytes</th>\n",
       "      <th>src_pkts</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>conn_state-OTH</th>\n",
       "      <th>conn_state-REJ</th>\n",
       "      <th>conn_state-RSTO</th>\n",
       "      <th>...</th>\n",
       "      <th>service-ssl</th>\n",
       "      <th>dns_AA--1</th>\n",
       "      <th>dns_AA-F</th>\n",
       "      <th>dns_AA-T</th>\n",
       "      <th>dns_RA--1</th>\n",
       "      <th>dns_RA-F</th>\n",
       "      <th>dns_RA-T</th>\n",
       "      <th>dns_RD--1</th>\n",
       "      <th>dns_RD-F</th>\n",
       "      <th>dns_RD-T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3786</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4765</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6112</td>\n",
       "      <td>800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5259</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>2790</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46559</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12284</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46560</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8462</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>ddos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46561</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3631</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46562</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>9338</td>\n",
       "      <td>712</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46563</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6528</td>\n",
       "      <td>349</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46564 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dns_qtype  dns_rcode  dns_query  dst_ip_bytes  src_pkts  label    type  \\\n",
       "0              0          0          2          3786         6      0  normal   \n",
       "1             12          0       4765           172         1      0  normal   \n",
       "2             12          3       6112           800         1      0  normal   \n",
       "3             43          0       5259           525         1      0  normal   \n",
       "4             43          0       2790           408         1      0  normal   \n",
       "...          ...        ...        ...           ...       ...    ...     ...   \n",
       "46559          1          0      12284           158         1      0  normal   \n",
       "46560          0          0          2          8462        10      1    ddos   \n",
       "46561         12          0       3631           375         1      0  normal   \n",
       "46562         28          0       9338           712         1      0  normal   \n",
       "46563         12          3       6528           349         2      0  normal   \n",
       "\n",
       "       conn_state-OTH  conn_state-REJ  conn_state-RSTO  ...  service-ssl  \\\n",
       "0                   0               0                0  ...            0   \n",
       "1                   0               0                0  ...            0   \n",
       "2                   0               0                0  ...            0   \n",
       "3                   0               0                0  ...            0   \n",
       "4                   0               0                0  ...            0   \n",
       "...               ...             ...              ...  ...          ...   \n",
       "46559               0               0                0  ...            0   \n",
       "46560               0               0                0  ...            1   \n",
       "46561               0               0                0  ...            0   \n",
       "46562               0               0                0  ...            0   \n",
       "46563               0               0                0  ...            0   \n",
       "\n",
       "       dns_AA--1  dns_AA-F  dns_AA-T  dns_RA--1  dns_RA-F  dns_RA-T  \\\n",
       "0              1         0         0          1         0         0   \n",
       "1              0         0         1          0         0         1   \n",
       "2              0         1         0          0         1         0   \n",
       "3              0         1         0          0         1         0   \n",
       "4              0         1         0          0         1         0   \n",
       "...          ...       ...       ...        ...       ...       ...   \n",
       "46559          0         0         1          0         1         0   \n",
       "46560          1         0         0          1         0         0   \n",
       "46561          0         1         0          0         1         0   \n",
       "46562          0         1         0          0         1         0   \n",
       "46563          0         1         0          0         1         0   \n",
       "\n",
       "       dns_RD--1  dns_RD-F  dns_RD-T  \n",
       "0              1         0         0  \n",
       "1              0         1         0  \n",
       "2              0         1         0  \n",
       "3              0         1         0  \n",
       "4              0         1         0  \n",
       "...          ...       ...       ...  \n",
       "46559          0         1         0  \n",
       "46560          1         0         0  \n",
       "46561          0         1         0  \n",
       "46562          0         1         0  \n",
       "46563          0         0         1  \n",
       "\n",
       "[46564 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./preprocessed.csv', low_memory=False) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dns_qtype</th>\n",
       "      <th>dns_rcode</th>\n",
       "      <th>dns_query</th>\n",
       "      <th>dst_ip_bytes</th>\n",
       "      <th>src_pkts</th>\n",
       "      <th>conn_state-OTH</th>\n",
       "      <th>conn_state-REJ</th>\n",
       "      <th>conn_state-RSTO</th>\n",
       "      <th>conn_state-RSTOS0</th>\n",
       "      <th>conn_state-RSTR</th>\n",
       "      <th>...</th>\n",
       "      <th>service-ssl</th>\n",
       "      <th>dns_AA--1</th>\n",
       "      <th>dns_AA-F</th>\n",
       "      <th>dns_AA-T</th>\n",
       "      <th>dns_RA--1</th>\n",
       "      <th>dns_RA-F</th>\n",
       "      <th>dns_RA-T</th>\n",
       "      <th>dns_RD--1</th>\n",
       "      <th>dns_RD-F</th>\n",
       "      <th>dns_RD-T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3786</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4765</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>6112</td>\n",
       "      <td>800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5259</td>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>2790</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dns_qtype  dns_rcode  dns_query  dst_ip_bytes  src_pkts  conn_state-OTH  \\\n",
       "0          0          0          2          3786         6               0   \n",
       "1         12          0       4765           172         1               0   \n",
       "2         12          3       6112           800         1               0   \n",
       "3         43          0       5259           525         1               0   \n",
       "4         43          0       2790           408         1               0   \n",
       "\n",
       "   conn_state-REJ  conn_state-RSTO  conn_state-RSTOS0  conn_state-RSTR  ...  \\\n",
       "0               0                0                  0                0  ...   \n",
       "1               0                0                  0                0  ...   \n",
       "2               0                0                  0                0  ...   \n",
       "3               0                0                  0                0  ...   \n",
       "4               0                0                  0                0  ...   \n",
       "\n",
       "   service-ssl  dns_AA--1  dns_AA-F  dns_AA-T  dns_RA--1  dns_RA-F  dns_RA-T  \\\n",
       "0            0          1         0         0          1         0         0   \n",
       "1            0          0         0         1          0         0         1   \n",
       "2            0          0         1         0          0         1         0   \n",
       "3            0          0         1         0          0         1         0   \n",
       "4            0          0         1         0          0         1         0   \n",
       "\n",
       "   dns_RD--1  dns_RD-F  dns_RD-T  \n",
       "0          1         0         0  \n",
       "1          0         1         0  \n",
       "2          0         1         0  \n",
       "3          0         1         0  \n",
       "4          0         1         0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['label','type'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.filter(items=['label'])\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336797</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.432005</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371713</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197201</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1         2         3         4    5    6    7    8    9   ...  \\\n",
       "0  0.000000  0.0  0.000141  0.000044  0.000024  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  0.047059  0.0  0.336797  0.000002  0.000004  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  0.047059  0.6  0.432005  0.000009  0.000004  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  0.168627  0.0  0.371713  0.000006  0.000004  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  0.168627  0.0  0.197201  0.000005  0.000004  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "    33   34   35   36   37   38   39   40   41   42  \n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "2  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "3  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "4  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "X =  pd.DataFrame(min_max_scaler.fit_transform(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(X.values) \n",
    "gt = torch.Tensor(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46564, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46564, 43])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "\n",
    "# Create a TensorDataset from your data and labels\n",
    "dataset = TensorDataset(x, gt)\n",
    "\n",
    "# Calculate the number of samples for training and testing datasets\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * 0.7)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "def load_data():\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(0, self.dim)\n",
    "        original_size = input.size()\n",
    "        input = input.reshape(input.size(0), -1)\n",
    "        input = input.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = input.size(dim)\n",
    "        \n",
    "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
    "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
    "        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n",
    "        range = range.expand_as(zs)\n",
    "\n",
    "        bound = 1 + range * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
    "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
    "        zs_sparse = is_gt * zs\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(input)\n",
    "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "        return output\n",
    "    def backward(self, grad_output):\n",
    "        dim = 1\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/implementing-tabnet-in-pytorch-fc977c383279\n",
    "# https://www.kaggle.com/code/samratthapa/tabnet-implementation?scriptVersionId=46472520\n",
    "\n",
    "\n",
    "class GBN(nn.Module):\n",
    "    def __init__(self,inp,vbs=128,momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "    def forward(self,x):\n",
    "        chunk = torch.chunk(x,max(1,x.size(0)//self.vbs),0)\n",
    "        res = [self.bn(y) for y in chunk ]\n",
    "        return torch.cat(res,0)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n",
    "        super().__init__()\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = nn.Linear(inp_dim,out_dim*2)\n",
    "        self.bn = GBN(out_dim*2,vbs=vbs) \n",
    "        self.od = out_dim\n",
    "    def forward(self,x):\n",
    "        x = self.bn(self.fc(x))\n",
    "        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n",
    "    \n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n",
    "        super().__init__()\n",
    "        first = True\n",
    "        self.shared = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n",
    "            first= False    \n",
    "            for fc in shared[1:]:\n",
    "                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        self.independ = nn.ModuleList()\n",
    "        if first:\n",
    "            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n",
    "        for x in range(first, n_ind):\n",
    "            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n",
    "    def forward(self,x):\n",
    "        if self.shared:\n",
    "            x = self.shared[0](x)\n",
    "            for glu in self.shared[1:]:\n",
    "                x = torch.add(x, glu(x))\n",
    "                x = x*self.scale\n",
    "                \n",
    "        for glu in self.independ:\n",
    "            x = torch.add(x, glu(x))\n",
    "            x = x*self.scale\n",
    "        return x\n",
    "\n",
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(inp_dim,out_dim)\n",
    "        self.bn = GBN(out_dim,vbs=vbs)\n",
    "#         self.smax = Sparsemax()\n",
    "        self.r = torch.tensor([relax],device=device)\n",
    "    def forward(self,a,priors):\n",
    "        a = self.bn(self.fc(a))\n",
    "        mask = torch.sigmoid(a*priors)\n",
    "        priors =priors*(self.r-mask)\n",
    "        return mask\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n",
    "        self.atten_tran = AttentionTransformer(n_a,inp_dim,relax,vbs)\n",
    "    def forward(self,x,a,priors):\n",
    "        mask = self.atten_tran(a,priors)\n",
    "        loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n",
    "        x = self.fea_tran(x*mask)\n",
    "        return x,loss\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self, inp_dim, final_out_dim=1, n_d=64, n_a=64, n_shared=2, n_ind=2, n_steps=3, relax=1.2, vbs=128, lambda_sparse=0.0001):\n",
    "        super().__init__()\n",
    "        self.lambda_sparse = lambda_sparse  # Adding sparse regularization weight\n",
    "        \n",
    "        if n_shared > 0:\n",
    "            self.shared = nn.ModuleList([nn.Linear(inp_dim, 2 * (n_d + n_a))])\n",
    "            for _ in range(1, n_shared):\n",
    "                self.shared.append(nn.Linear(n_d + n_a, 2 * (n_d + n_a)))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        \n",
    "        self.first_step = FeatureTransformer(inp_dim, n_d + n_a, self.shared, n_ind, vbs=vbs)\n",
    "        self.steps = nn.ModuleList()\n",
    "        for _ in range(n_steps - 1):\n",
    "            self.steps.append(DecisionStep(inp_dim, n_d, n_a, self.shared, n_ind, relax, vbs))\n",
    "        \n",
    "        self.fc = nn.Linear(n_d, final_out_dim)\n",
    "        self.bn = nn.BatchNorm1d(inp_dim)\n",
    "        self.n_d = n_d\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x_a = self.first_step(x)[:, self.n_d:]\n",
    "        sparse_loss = torch.tensor(0.).to(self.device)\n",
    "        out = torch.zeros(x.size(0), self.n_d).to(self.device)\n",
    "        priors = torch.ones(x.shape).to(self.device)\n",
    "        for step in self.steps:\n",
    "            x_te, l = step(x, x_a, priors)\n",
    "            out += F.relu(x_te[:, :self.n_d])\n",
    "            x_a = x_te[:, self.n_d:]\n",
    "            sparse_loss += l\n",
    "        \n",
    "        # Final activation function changed to sigmoid for binary classification\n",
    "        out = torch.sigmoid(self.fc(out))\n",
    "        # Add the scaled sparse_loss to the main loss outside of this method during optimization\n",
    "        return out, sparse_loss * self.lambda_sparse\n",
    "    \n",
    "class TabNetWithEmbed(nn.Module):\n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n",
    "        super().__init__()\n",
    "        self.tabnet = TabNet(inp_dim,final_out_dim,n_d,n_a,n_shared,n_ind,n_steps,relax,vbs)\n",
    "        self.cat_embed = []\n",
    "        self.emb1 = nn.Embedding(2,1)\n",
    "        self.emb3 = nn.Embedding(3,1)\n",
    "        self.cat_embed.append(self.emb1)\n",
    "        self.cat_embed.append(self.emb3)\n",
    "        \n",
    "    def forward(self,catv,contv):\n",
    "        catv = catv.to(device)\n",
    "        contv = contv.to(device)\n",
    "        embeddings = [embed(catv[:,idx]) for embed,idx in zip(self.cat_embed,range(catv.size(1)))]\n",
    "        catv = torch.cat(embeddings,1)\n",
    "        x = torch.cat((catv,contv),1).contiguous()\n",
    "        x,l = self.tabnet(x)\n",
    "        return torch.sigmoid(x),l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabnet(config):\n",
    "    net = TabNet(43, n_d=config[\"n_d\"], n_a=config[\"n_a\"], n_shared=config[\"n_shared\"], n_ind=config[\"n_ind\"], n_steps=config[\"n_steps\"], relax=config[\"relax\"], vbs=config[\"vbs\"], lambda_sparse=config[\"lambda_sparse\"]).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if \"restore\" in config:\n",
    "        checkpoint_path = config[\"restore\"]\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    trainset, _ = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.7)\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    trainloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=0)\n",
    "    valloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=0)\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, sparse_loss = net(inputs)\n",
    "            loss = criterion(outputs, labels) + 1e-4 * sparse_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs, sparse_loss = net(inputs)\n",
    "                preds = outputs.round()\n",
    "                total_correct += preds.eq(labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                loss = criterion(outputs, labels) + 1e-4 * sparse_loss\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Calculate overall validation accuracy\n",
    "        accuracy = (total_correct / total_samples) * 100\n",
    "                \n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            checkpoint_path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save({\n",
    "                \"model_state_dict\": net.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            # Use ray.train.report to report metrics and checkpoint\n",
    "            ray.train.report({\n",
    "                \"loss\": val_loss,\n",
    "                \"accuracy\": accuracy\n",
    "            }, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))\n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_result):\n",
    "    best_trained_model = TabNet(43, n_d=best_result.config[\"n_d\"], n_a=best_result.config[\"n_a\"], n_shared=best_result.config[\"n_shared\"], n_ind=best_result.config[\"n_ind\"], n_steps=best_result.config[\"n_steps\"], relax=best_result.config[\"relax\"], vbs=best_result.config[\"vbs\"], lambda_sparse=best_result.config[\"lambda_sparse\"]).to(device)\n",
    "    \n",
    "    print(f'> Number of parameters {len(torch.nn.utils.parameters_to_vector(best_trained_model.parameters()))}')\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    _, testset = load_data()\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, loss = best_trained_model(inputs)\n",
    "            predicted = outputs.round()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Best trial test set accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-22 19:36:19</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:42.82        </td></tr>\n",
       "<tr><td>Memory:      </td><td>144.4/1132.4 GiB   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=6<br>Bracket: Iter 64.000: -2.5323836877942085 | Iter 32.000: -2.0915487706661224 | Iter 16.000: -2.225994251668453 | Iter 8.000: -2.3707662001252174 | Iter 4.000: -2.453717064112425 | Iter 2.000: -3.540981851518154 | Iter 1.000: -12.83631306886673<br>Logical resource usage: 64.0/64 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  lambda_sparse</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_a</th><th style=\"text-align: right;\">  n_d</th><th style=\"text-align: right;\">  n_ind</th><th style=\"text-align: right;\">  n_shared</th><th style=\"text-align: right;\">  n_steps</th><th style=\"text-align: right;\">   relax</th><th style=\"text-align: right;\">  vbs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tabnet_da096_00006</td><td>RUNNING   </td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    9.80383e-06</td><td style=\"text-align: right;\">0.0245868  </td><td style=\"text-align: right;\">   59</td><td style=\"text-align: right;\">   23</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">1.18977 </td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        50.3718 </td><td style=\"text-align: right;\"> 2.00027</td><td style=\"text-align: right;\">   95.3369</td></tr>\n",
       "<tr><td>train_tabnet_da096_00007</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    7.46545e-06</td><td style=\"text-align: right;\">0.0934418  </td><td style=\"text-align: right;\">   62</td><td style=\"text-align: right;\">   43</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.648997</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_tabnet_da096_00008</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    8.74576e-06</td><td style=\"text-align: right;\">4.07895e-05</td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">   11</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">0.82598 </td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_tabnet_da096_00009</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    7.85676e-06</td><td style=\"text-align: right;\">0.00998986 </td><td style=\"text-align: right;\">   36</td><td style=\"text-align: right;\">   17</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.706382</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_tabnet_da096_00000</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    4.43176e-07</td><td style=\"text-align: right;\">0.066866   </td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">    3</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.21678 </td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       261.662  </td><td style=\"text-align: right;\"> 2.14865</td><td style=\"text-align: right;\">   94.6723</td></tr>\n",
       "<tr><td>train_tabnet_da096_00001</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    7.36724e-06</td><td style=\"text-align: right;\">0.0278469  </td><td style=\"text-align: right;\">   60</td><td style=\"text-align: right;\">   52</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">1.10613 </td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.22821</td><td style=\"text-align: right;\">19.439  </td><td style=\"text-align: right;\">   93.844 </td></tr>\n",
       "<tr><td>train_tabnet_da096_00002</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    8.62619e-06</td><td style=\"text-align: right;\">0.0427846  </td><td style=\"text-align: right;\">   13</td><td style=\"text-align: right;\">   51</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.755378</td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.43128</td><td style=\"text-align: right;\">29.8393 </td><td style=\"text-align: right;\">   86.9619</td></tr>\n",
       "<tr><td>train_tabnet_da096_00003</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    9.8738e-06 </td><td style=\"text-align: right;\">0.0719789  </td><td style=\"text-align: right;\">    9</td><td style=\"text-align: right;\">   18</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.889047</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.26494</td><td style=\"text-align: right;\">59.5541 </td><td style=\"text-align: right;\">   86.8596</td></tr>\n",
       "<tr><td>train_tabnet_da096_00004</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    7.6654e-06 </td><td style=\"text-align: right;\">0.0776692  </td><td style=\"text-align: right;\">    6</td><td style=\"text-align: right;\">   20</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.502163</td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.34081</td><td style=\"text-align: right;\"> 3.90351</td><td style=\"text-align: right;\">   86.7369</td></tr>\n",
       "<tr><td>train_tabnet_da096_00005</td><td>TERMINATED</td><td>10.234.204.81:172508</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    8.48037e-06</td><td style=\"text-align: right;\">6.23585e-05</td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">   45</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.558263</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.22889</td><td style=\"text-align: right;\">12.8306 </td><td style=\"text-align: right;\">   86.9107</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 19:30:42,568\tWARNING worker.py:2065 -- Warning: The actor ImplicitFunc is very large (15 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=100, gpus_per_trial=1):\n",
    "    config = {\n",
    "        \"n_d\": tune.randint(2, 128 + 1),\n",
    "        \"n_a\": tune.randint(2, 128 + 1),\n",
    "        \"n_shared\": tune.randint(1, 3 + 1),\n",
    "        \"n_ind\": tune.randint(1, 3 + 1),\n",
    "        \"n_steps\": tune.randint(1, 3 + 1),\n",
    "        \"relax\": tune.uniform(0.1, 1.2),\n",
    "        \"vbs\": tune.choice([16, 32, 64, 128]),\n",
    "        \"lambda_sparse\": tune.uniform(0, 0.00001),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"batch_size\": tune.choice([64, 128, 256, 512, 1024]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=False)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_tabnet),\n",
    "            resources={\"cpu\": 64, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(\n",
    "            local_dir=local_dir,\n",
    "            name=\"tabnet\",\n",
    "        )\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result)\n",
    "    \n",
    "main(num_samples=10, max_num_epochs=100, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.filter(items=['type'])\n",
    "y['type'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/tghv73/myjupyterenv/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_int = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(X.values) \n",
    "gt = torch.Tensor(y_int).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "\n",
    "# Create a TensorDataset from your data and labels\n",
    "dataset = TensorDataset(x, gt)\n",
    "\n",
    "# Calculate the number of samples for training and testing datasets\n",
    "num_samples = len(dataset)\n",
    "train_size = int(num_samples * 0.7)\n",
    "test_size = num_samples - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetWithEmbed(nn.Module):\n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n",
    "        super().__init__()\n",
    "        self.tabnet = TabNet(inp_dim,final_out_dim,n_d,n_a,n_shared,n_ind,n_steps,relax,vbs)\n",
    "        self.cat_embed = []\n",
    "        self.emb1 = nn.Embedding(2,1)\n",
    "        self.emb3 = nn.Embedding(3,1)\n",
    "        self.cat_embed.append(self.emb1)\n",
    "        self.cat_embed.append(self.emb3)\n",
    "        \n",
    "    def forward(self,catv,contv):\n",
    "        catv = catv.to(device)\n",
    "        contv = contv.to(device)\n",
    "        embeddings = [embed(catv[:,idx]) for embed,idx in zip(self.cat_embed,range(catv.size(1)))]\n",
    "        catv = torch.cat(embeddings,1)\n",
    "        x = torch.cat((catv,contv),1).contiguous()\n",
    "        x,l = self.tabnet(x)\n",
    "        return torch.softmax(x, dim=1),l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabnet(config):\n",
    "    net = TabNet(43, 10, n_d=config[\"n_d\"], n_a=config[\"n_a\"], n_shared=config[\"n_shared\"], n_ind=config[\"n_ind\"], n_steps=config[\"n_steps\"], relax=config[\"relax\"], vbs=config[\"vbs\"], lambda_sparse=config[\"lambda_sparse\"]).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if \"restore\" in config:\n",
    "        checkpoint_path = config[\"restore\"]\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    trainset, _ = load_data()\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.7)\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    trainloader = DataLoader(train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=0)\n",
    "    valloader = DataLoader(val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=0)\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, sparse_loss = net(inputs)\n",
    "            loss = criterion(outputs, labels) + 1e-4 * sparse_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs, sparse_loss = net(inputs)\n",
    "                _, argmax = torch.max(outputs, dim=1)\n",
    "                total_correct += argmax.eq(labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                loss = criterion(outputs, labels) + 1e-4 * sparse_loss\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Calculate overall validation accuracy\n",
    "        accuracy = (total_correct / total_samples) * 100\n",
    "                \n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            checkpoint_path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save({\n",
    "                \"model_state_dict\": net.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            # Use ray.train.report to report metrics and checkpoint\n",
    "            ray.train.report({\n",
    "                \"loss\": val_loss,\n",
    "                \"accuracy\": accuracy\n",
    "            }, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))\n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_result):\n",
    "    best_trained_model = TabNet(43, 10, n_d=best_result.config[\"n_d\"], n_a=best_result.config[\"n_a\"], n_shared=best_result.config[\"n_shared\"], n_ind=best_result.config[\"n_ind\"], n_steps=best_result.config[\"n_steps\"], relax=best_result.config[\"relax\"], vbs=best_result.config[\"vbs\"], lambda_sparse=best_result.config[\"lambda_sparse\"]).to(device)\n",
    "    \n",
    "    print(f'> Number of parameters {len(torch.nn.utils.parameters_to_vector(best_trained_model.parameters()))}')\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    _, testset = load_data()\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, sparse_loss = best_trained_model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Best trial test set accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-19 20:03:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:16:44.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>95.3/1132.4 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 64.000: -15.243484616279602 | Iter 32.000: -15.297015309333801 | Iter 16.000: -15.282134652137756 | Iter 8.000: -15.309714674949646 | Iter 4.000: -15.361475110054016 | Iter 2.000: -22.841317057609558 | Iter 1.000: -42.30851227045059<br>Logical resource usage: 64.0/64 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  lambda_sparse</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_a</th><th style=\"text-align: right;\">  n_d</th><th style=\"text-align: right;\">  n_ind</th><th style=\"text-align: right;\">  n_shared</th><th style=\"text-align: right;\">  n_steps</th><th style=\"text-align: right;\">   relax</th><th style=\"text-align: right;\">  vbs</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tabnet_7ac47_00000</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    7.18292e-06</td><td style=\"text-align: right;\">0.014565   </td><td style=\"text-align: right;\">   47</td><td style=\"text-align: right;\">   27</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.107751</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       142.629  </td><td style=\"text-align: right;\"> 30.3911</td><td style=\"text-align: right;\">  91.6556 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00001</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    7.30056e-06</td><td style=\"text-align: right;\">0.000599778</td><td style=\"text-align: right;\">  102</td><td style=\"text-align: right;\">   76</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.660844</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.76008</td><td style=\"text-align: right;\">235.14  </td><td style=\"text-align: right;\">  91.7578 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00002</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    3.20049e-06</td><td style=\"text-align: right;\">0.00251295 </td><td style=\"text-align: right;\">   37</td><td style=\"text-align: right;\">  111</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.851933</td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       163.207  </td><td style=\"text-align: right;\"> 15.1579</td><td style=\"text-align: right;\">  92.443  </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00003</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    3.46313e-06</td><td style=\"text-align: right;\">0.000230229</td><td style=\"text-align: right;\">    9</td><td style=\"text-align: right;\">   40</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.63953 </td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.32998</td><td style=\"text-align: right;\">350.917 </td><td style=\"text-align: right;\">   1.59526</td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00004</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    1.17652e-06</td><td style=\"text-align: right;\">2.8585e-05 </td><td style=\"text-align: right;\">   57</td><td style=\"text-align: right;\">  119</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">1.03872 </td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       163.289  </td><td style=\"text-align: right;\"> 15.3126</td><td style=\"text-align: right;\">  92.0851 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00005</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    9.80669e-06</td><td style=\"text-align: right;\">0.0455735  </td><td style=\"text-align: right;\">   25</td><td style=\"text-align: right;\">   76</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.809862</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.85169</td><td style=\"text-align: right;\">236.595 </td><td style=\"text-align: right;\">  86.6039 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00006</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    4.76241e-06</td><td style=\"text-align: right;\">6.10794e-05</td><td style=\"text-align: right;\">   98</td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">1.06731 </td><td style=\"text-align: right;\">   64</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.35386</td><td style=\"text-align: right;\"> 22.1703</td><td style=\"text-align: right;\">  12.0667 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00007</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    7.09905e-06</td><td style=\"text-align: right;\">0.00671051 </td><td style=\"text-align: right;\">   29</td><td style=\"text-align: right;\">   58</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">0.628315</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.08177</td><td style=\"text-align: right;\">235.394 </td><td style=\"text-align: right;\">  89.682  </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00008</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    8.05662e-06</td><td style=\"text-align: right;\">0.000233435</td><td style=\"text-align: right;\">   87</td><td style=\"text-align: right;\">   96</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">1.08061 </td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.35065</td><td style=\"text-align: right;\">349.948 </td><td style=\"text-align: right;\">   6.58554</td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00009</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    2.13959e-06</td><td style=\"text-align: right;\">0.000792404</td><td style=\"text-align: right;\">   63</td><td style=\"text-align: right;\">   84</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.26863 </td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.1567 </td><td style=\"text-align: right;\">336.925 </td><td style=\"text-align: right;\">  86.6858 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00010</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    2.23775e-06</td><td style=\"text-align: right;\">0.0072424  </td><td style=\"text-align: right;\">  108</td><td style=\"text-align: right;\">  122</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">0.964882</td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       217.478  </td><td style=\"text-align: right;\"> 15.2241</td><td style=\"text-align: right;\">  90.2955 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00011</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    6.86697e-06</td><td style=\"text-align: right;\">0.00297781 </td><td style=\"text-align: right;\">   10</td><td style=\"text-align: right;\">   80</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">1.19426 </td><td style=\"text-align: right;\">   16</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       251.006  </td><td style=\"text-align: right;\"> 15.1675</td><td style=\"text-align: right;\">  92.3101 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00012</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">    6.0611e-06 </td><td style=\"text-align: right;\">1.76078e-05</td><td style=\"text-align: right;\">   82</td><td style=\"text-align: right;\">   68</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.167968</td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.95404</td><td style=\"text-align: right;\"> 89.2428</td><td style=\"text-align: right;\">  86.4506 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00013</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    2.26928e-07</td><td style=\"text-align: right;\">0.0142777  </td><td style=\"text-align: right;\">   56</td><td style=\"text-align: right;\">  102</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.698803</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.76275</td><td style=\"text-align: right;\"> 15.378 </td><td style=\"text-align: right;\">  91.0318 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00014</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    6.11992e-06</td><td style=\"text-align: right;\">1.5363e-05 </td><td style=\"text-align: right;\">   51</td><td style=\"text-align: right;\">  115</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.255291</td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.68669</td><td style=\"text-align: right;\"> 45.3899</td><td style=\"text-align: right;\">   7.20933</td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00015</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    2.1318e-06 </td><td style=\"text-align: right;\">0.000835235</td><td style=\"text-align: right;\">   85</td><td style=\"text-align: right;\">   20</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">1.01153 </td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.71695</td><td style=\"text-align: right;\"> 34.5333</td><td style=\"text-align: right;\">  91.042  </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00016</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">    8.27478e-06</td><td style=\"text-align: right;\">0.037303   </td><td style=\"text-align: right;\">  127</td><td style=\"text-align: right;\">   77</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">0.520552</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.648  </td><td style=\"text-align: right;\"> 30.896 </td><td style=\"text-align: right;\">  86.788  </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00017</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    2.28365e-06</td><td style=\"text-align: right;\">0.0336281  </td><td style=\"text-align: right;\">  109</td><td style=\"text-align: right;\">  109</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.894888</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.6705 </td><td style=\"text-align: right;\">236.729 </td><td style=\"text-align: right;\">  86.7471 </td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00018</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\">    7.52538e-06</td><td style=\"text-align: right;\">2.04343e-05</td><td style=\"text-align: right;\">   32</td><td style=\"text-align: right;\">   28</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">0.521827</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.54484</td><td style=\"text-align: right;\"> 23.5123</td><td style=\"text-align: right;\">   2.11678</td></tr>\n",
       "<tr><td>train_tabnet_7ac47_00019</td><td>TERMINATED</td><td>10.234.204.81:3971489</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">    6.08857e-06</td><td style=\"text-align: right;\">0.000766815</td><td style=\"text-align: right;\">   34</td><td style=\"text-align: right;\">   80</td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        2</td><td style=\"text-align: right;\">0.657888</td><td style=\"text-align: right;\">  128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.37844</td><td style=\"text-align: right;\"> 62.2774</td><td style=\"text-align: right;\">  91.4102 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 19:47:18,235\tWARNING worker.py:2065 -- Warning: The actor ImplicitFunc is very large (16 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "2024-03-19 20:03:56,488\tINFO tune.py:1154 -- Total run time: 1004.34 seconds (1004.10 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'n_d': 111, 'n_a': 37, 'n_shared': 2, 'n_ind': 1, 'n_steps': 3, 'relax': 0.8519332861691487, 'vbs': 32, 'lambda_sparse': 3.2004898522373206e-06, 'lr': 0.0025129532017995728, 'batch_size': 1024}\n",
      "Best trial final validation loss: 15.157926559448242\n",
      "Best trial final validation accuracy: 92.44299008078536\n",
      "> Number of parameters 199414\n",
      "Best trial test set accuracy: 91.45311381531855%\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=100, gpus_per_trial=1):\n",
    "    config = {\n",
    "        \"n_d\": tune.randint(2, 128 + 1),\n",
    "        \"n_a\": tune.randint(2, 128 + 1),\n",
    "        \"n_shared\": tune.randint(1, 3 + 1),\n",
    "        \"n_ind\": tune.randint(1, 3 + 1),\n",
    "        \"n_steps\": tune.randint(1, 3 + 1),\n",
    "        \"relax\": tune.uniform(0.1, 1.2),\n",
    "        \"vbs\": tune.choice([16, 32, 64, 128]),\n",
    "        \"lambda_sparse\": tune.uniform(0, 0.00001),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"batch_size\": tune.choice([64, 128, 256, 512, 1024]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=False)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_tabnet),\n",
    "            resources={\"cpu\": 64, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=air.RunConfig(\n",
    "            local_dir=local_dir,\n",
    "            name=\"tabnet\",\n",
    "        )\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result)\n",
    "    \n",
    "main(num_samples=20, max_num_epochs=100, gpus_per_trial=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
